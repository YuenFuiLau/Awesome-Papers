# Awesome-Papers


---

<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome Papers](#awesome-papers)
- - [Awesome Suvey](#Awesome-Suvey)
  - [Awesome Robot Learning](#Awesome-Robot-Learning)
  - [Awesome Imitation Learning](#Awesome-Imitation-Learning)
  - [Awesome Vision](#Awesome-Vision)
  - [Awesome Teleoperation](#Awesome-Teleoperation)
  - [Awesome Locomoation](#Awesome-Locomoation)
  - [Awesome Learning](#Awesome-Learning)
  - [Awesome Meta-Learning/Increamental Learniing/Continuous Learning](#Awesome-Meta-Learning/Increamental-Learniing/Continuous-Learning)
  - [Awesome Data](#Awesome-Data)


- [Awesome Reference](#awesome-reference-source)
  - [Awesome Books](#Awesome-books)
  - [Awesome Source](#Awesome-source)
---

# Awesome Papers

## Awesome Suvey
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning**](https://arxiv.org/abs/2501.02116) <br> | arVix  | 2025-01 |  |  |
| <br> [**A Survey of Meta-Reinforcement Learning**](https://arxiv.org/abs/2301.08028v3) <br> | arVix  | 2024 |  |  |
| <br> [**Teleoperation of Humanoid Robots: A Survey**](https://arxiv.org/pdf/2301.04317) <br> | arXiv | 2023-01-11 | [Github.io](https://humanoid-teleoperation.github.io/)  |  |
| <br> [**LLM Post-Training: A Deep Dive into Reasoning**](https://arxiv.org/abs/2502.21321) <br> | arVix  | 2025 |  |  |

## Awesome Robot Learning
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**VideoMimic Visual imitation enables contextual humanoid control**](https://www.videomimic.net/) <br> | arXiv  | 2025 | [Code](https://github.com/hongsukchoi/VideoMimic) |  |
| <br> [**AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems**](https://agibot-world.com/) <br> | arVix  | 2025 | [Code](https://github.com/OpenDriveLab/AgiBot-World) |  |
| <br> [**π0: A Vision-Language-Action Flow Model for General Robot Control**](https://www.physicalintelligence.company/blog/pi0) <br> | arVix  | 2024 | [Code](https://github.com/allenzren/open-pi-zero) |  |
| <br> [**INTERMIMIC: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions**](https://arxiv.org/pdf/2502.20390) <br> | CVPR  | 2025 | [Code](https://github.com/Sirui-Xu/InterMimic) |  |
| <br> [**Controllable Human-Object Interaction Synthesis**](https://lijiaman.github.io/projects/chois/) <br> | ECCV  | 2024 | [Code](https://github.com/lijiaman/chois_release) |  |
| <br> [**Human-Object Interaction from Human-Level Instructions**](https://hoifhli.github.io/) <br> | arVix  | 2024-12 |  |  |
| <br> [**CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation**](https://cogact.github.io/) <br> | arXiv  | 2024-11 | [Code](https://github.com/microsoft/CogACT/tree/main) |  |
| <br> [**Visual Whole-Body Control for Legged Loco-Manipulation**](https://wholebody-b1.github.io/) <br> | CoRL | 2024-11-02 | [Code](https://github.com/Ericonaldo/visual_wholebody)  |  |
| <br> [**OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics**](https://ok-robot.github.io/) <br> | CoRL | 2024-02-29 | [Code](https://github.com/ok-robot/ok-robot)  |  |
| <br> [**OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation**](https://ut-austin-rpl.github.io/OKAMI/) <br> | CoRL | 2024-09 |  |  |
| <br> [**CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics**](https://gao-jiawei.com/Research/CooHOI/) <br> | NeurIPS | 2024-10 | [Code](https://gao-jiawei.com/Research/CooHOI/) |  |
| <br> [**ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters**](https://heyuanyao-pku.github.io/Control-VAE/) <br> | SIGGRAPH Asia  | 2022| [Code](https://github.com/heyuanYao-pku/Control-VAE) |  |
| <br> [**PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction**](https://wyhuai.github.io/physhoi-page/) <br> | arXiv  | 2024-10 | [Code](https://github.com/wyhuai/PhysHOI/tree/main) |  |
| <br> [**SkillMimic: Learning Reusable Basketball Skills from Demonstrations**](https://ingrid789.github.io/SkillMimic/) <br> | CVPR  | 2024-08 | [Code](https://github.com/wyhuai/SkillMimic) |  |
| <br> [**Humanoid Policy ~ Human Policy**](https://human-as-robot.github.io/) <br> | arVix  | 2025| [Code](https://github.com/RogerQi/human-policy) |  |
| <br> [**DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies**](https://dexwild.github.io/) <br> | RSS | 2025|  |  |
| <br> [**Catch It! Learning to Catch in Flight with Mobile Dexterous Hands**](https://mobile-dex-catch.github.io/) <br> | ICRA | 2025 | [Code](https://github.com/hang0610/Catch_It) |  |
| <br> [**Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance**](https://arxiv.org/pdf/2505.18793) <br> | arXiv  | 2025 | |  |
| <br> [**Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space**](https://zzk273.github.io/R2S2/) <br> | arXiv | 2025 | [Code](https://github.com/GalaxyGeneralRobotics/OpenWBT) |  |
| <br> [**LAPA: Latent Action Pretraining from Videos**](https://latentactionpretraining.github.io/) <br> | ICLR | 2025 | [Code](https://github.com/LatentActionPretraining/LAPA) |  |
| <br> [**Humanoid Whole-Body Control with Latent Vision-Language Instruction**](https://ember-lab-berkeley.github.io/LeVERB-Website/) <br> | arXiv | 2025 | |  |


## Awesome Imitation Learning
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill**](https://jiemingcui.github.io/grove/) <br> | CVPR  | 2025 | [Code](https://github.com/jiemingcui/GROVE-pytorch) |  |
| <br> [**DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills**](https://xbpeng.github.io/projects/DeepMimic/index.html) <br> | ACM SIGGRAPH | 2024-05-22 | [Code](https://github.com/xbpeng/DeepMimic/tree/master)  |  |
| <br> [**AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control**](https://xbpeng.github.io/projects/AMP/index.html) <br> | ACM SIGGRAPH | 2024-05-22 | [Code1](https://github.com/Alescontrela/AMP_for_hardware) [Code2](https://github.com/nv-tlabs/ASE/tree/main)   |  |
| <br> [**ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters**](https://xbpeng.github.io/projects/ASE/index.html) <br> | ACM SIGGRAPH 2022 | 2024-05-22 | [Code](https://github.com/nv-tlabs/ASE/tree/main)  |  |
| <br> [**PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors**](https://github.com/jinseokbae/pmp) <br> | SIGGRAPH | 2023-05 |  |  |
| <br> [**C·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters**](https://frank-zy-dou.github.io/projects/CASE/index.html) <br> | SIGGRAPH ASIA 2023 | 2023-09 |  |  |
| <br> [**Synthesizing Physical Character-Scene Interactions**](https://research.nvidia.com/publication/2023-08_synthesizing-physical-character-scene-interactions) <br> | SIGGRAPH 2023 | 2023-02 |  |  |
| <br> [**TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization**](https://liangpan99.github.io/TokenHSI/) <br> | CVPR  | 2025 | [Code](https://github.com/liangpan99/TokenHSI) |  |
| <br> [**MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting**](https://research.nvidia.com/labs/par/maskedmimic/) <br> | SIGGRAPH Asia 2024 | 2024-09 | [Code](https://github.com/NVlabs/ProtoMotions) |  |
| <br> [**Learning Physically Simulated Tennis Skills from Broadcast Videos**](https://research.nvidia.com/labs/toronto-ai/vid2player3d/) <br> | SIGGRAPH | 2023 | [Code](https://github.com/nv-tlabs/vid2player3d) |  |
| <br> [**Perpetual Humanoid Control for Real-time Simulated Avatars**](https://zhengyiluo.github.io/PHC/) <br> | ICCV  | 2023 | [Code](https://github.com/ZhengyiLuo/PHC) |  |
| <br> [**Universal Humanoid Motion Representations for Physics-Based Control**](https://www.zhengyiluo.com/PULSE-Site/) <br> | ICLR  | 2024 | [Code](https://github.com/ZhengyiLuo/PULSE) |  |
| <br> [**Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking**](https://mimicking-bench.github.io/) <br> | arXiv  | 2024 | |  |

## Awesome Vision
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**HybrIK: Hybrid Analytical-Neural Inverse Kinematics for Body Mesh Recovery**](https://github.com/jeffffffli/HybrIK) <br> | CVPR | 2021 | [Code](https://github.com/jeffffffli/HybrIK)  |  |
| <br> [**Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions**](https://wenboran2002.github.io/3dhoi/) <br> | CVPR | 2025 | [Code](https://github.com/wenboran2002/open-3dhoi)  |  |
| <br> [**Probabilistic Representations for Video Contrastive Learning**](https://arxiv.org/abs/2204.03946) <br> | CVPR  | 2022 | [Code](https://github.com/mkirchhof/Probabilistic_Contrastive_Learning) |  |
| <br> [**TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos**](https://yufu-wang.github.io/tram4d/) <br> | ECCV  | 2024 | [Code](https://github.com/yufu-wang/tram) |  |
| <br> [**Capturing and Inferring Dense Full-Body Human-Scene Contact**](https://github.com/paulchhuang/bstro) <br> | CVPR  | 2022 | [Code](https://github.com/paulchhuang/bstro) |  |
| <br> [**Capturing and Inferring Dense Full-Body Human-Scene Contact**](https://github.com/paulchhuang/bstro) <br> | CVPR  | 2022 | [Code](https://github.com/paulchhuang/bstro) |  |

## Awesome Teleoperation
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit**](https://homietele.github.io/) <br> | RSS | 2025 | [Github.io](https://github.com/OpenRobotLab/OpenHomie)  |  |
| <br> [**IRIS: An Immersive Robot Interaction System**](https://arxiv.org/pdf/2502.03297) <br> | arXiv | 2025-02 |  |  |
| <br> [**Interactive Hand Pose Estimation using a Stretch-Sensing Soft Glove**](https://dl.acm.org/doi/pdf/10.1145/3306346.3322957) <br> | ACM Trans | 2019-07-12 |   |  |
| <br> [**Open-TeleVision: Teleoperation with Immersive Active Visual Feedback**](https://robot-tv.github.io/) <br> | CoRL | 2019-07-12 | [Code](https://github.com/OpenTeleVision/TeleVision)  |  |
| <br> [**AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System**](https://yzqin.github.io/anyteleop/) <br> | RSS | 2024-05-16 | [Code](https://github.com/dexsuite/dex-retargeting)  |  |
| <br> [**OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning**](https://omni.human2humanoid.com/) <br> | CoRL | 2024-01-13 | [Data](https://cmu.app.box.com/s/kmayzq5ax2rxvwn97s0hzz0aq5vws9io)  |  |
| <br> [**Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation**](https://human2humanoid.com/resources/H2O_paper.pdf) <br> | IROS | 2024-01-13 | [Github.io](https://human2humanoid.com/)  |  |

## Awesome Locomoation
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds**](https://why618188.github.io/beamdojo/) <br> | RSS | 2025 |  |  |
| <br> [**Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning**](https://www.roboticsproceedings.org/rss20/p058.pdf) <br> | RSS | 2024-07 |  |  |
| <br> [**Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer**](https://arxiv.org/pdf/2404.05695) <br> | arXiv | 2024-05-18 | [Code](https://github.com/roboterax/humanoid-gym) |  |
| <br> [**Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing**](https://lecar-lab.github.io/dial-mpc/) <br> | arXiv | 2024-05-18 | [Code](https://github.com/LeCAR-Lab/dial-mpc) |  |
| <br> [**Expressive Whole-Body Control for Humanoid Robots**](https://expressive-humanoid.github.io/) <br> | RSS | 2024-03-06 | [Code](https://github.com/chengxuxin/expressive-humanoid/tree/main) |  |
| <br> [**Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models**](https://metamotivo.metademolab.com/) <br> | arXiv  | 2024-12-12 | [Code](https://github.com/facebookresearch/metamotivo) |  |
| <br> [**ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills**](https://agile.human2humanoid.com/) <br> | RSS | 2025-02 | [Code](https://github.com/LeCAR-Lab/ASAP) |  |
| <br> [**HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots**](https://hover-versatile-humanoid.github.io/) <br> | ICRA | 2024-06-13 | [Code](https://github.com/NVlabs/HOVER/)   |  |
| <br> [**Learning Getting-Up Policies for Real-World Humanoid Robots**](https://humanoid-getup.github.io/) <br> | RSS | 2025 | [Code](https://github.com/RunpeiDong/humanup) | |
| <br> [**Learning Humanoid Standing-up Control across Diverse Postures**](https://taohuang13.github.io/humanoid-standingup.github.io/) <br> | RSS | 2025 | [Code](https://github.com/OpenRobotLab/HoST) | |
| <br> [**Learning by cheating**](https://github.com/dotchen/LearningByCheating) <br> | CoRL  | 2019 | [Code](https://github.com/dotchen/LearningByCheating)  |  |
| <br> [**Learning Quadrupedal Locomotion over Challenging Terrain**](https://leggedrobotics.github.io/rl-blindloco/) <br> | Sci. Robotics  | 2020 | |  |
| <br> [**KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills**](https://kungfu-bot.github.io/) <br> | arXiv | 2025 | [Code](https://github.com/TeleHuman/PBHC) |  |

## Awesome Learning
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**Rethinking Minimal Sufficient Representation in Contrastive Learning**](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Rethinking_Minimal_Sufficient_Representation_in_Contrastive_Learning_CVPR_2022_paper.pdf) <br> | CVPR | 2022 | [Code](https://github.com/Haoqing-Wang/InfoCL)  |  |
| <br> [**Variational Information Distillation for Knowledge Transfer**](https://arxiv.org/pdf/1904.05835) <br> | CVPR | 2019 |  |  |

## Awesome Meta-Learning/Increamental Learniing/Continuous Learning
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**Online Task-Free Continual Generative and Discriminative Learning via Dynamic Cluster Memory**](https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_Online_Task-Free_Continual_Generative_and_Discriminative_Learning_via_Dynamic_Cluster_CVPR_2024_paper.pdf) <br> | CVPR | 2025 | |  |
| <br> [**ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy**](https://cccedric.github.io/conrft/) <br> | RSS  | 2025 | [Code](https://github.com/cccedric/conrft) |  |

## Awesome Generation
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation**](https://awfuact.github.io/zerohsi/) <br> | arXiv | 2025 |  |  |
| <br> [**One-step Diffusion with Distribution Matching Distillation**](https://tianweiy.github.io/dmd/) <br> | CVPR | 2024 | [Code](https://github.com/devrimcavusoglu/dmd)  |  |
| <br> [**Improved Distribution Matching Distillation for Fast Image Synthesis**](https://tianweiy.github.io/dmd2/) <br> | NeurIPS | 2024 | [Code](https://github.com/tianweiy/DMD2)  |  |
| <br> [**SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS**](https://arxiv.org/pdf/2011.13456) <br> | ICLR | 2021 | [Code](https://github.com/yang-song/score_sde)  |  |
| <br> [**Image-to-Image Translation with Conditional Adversarial Networks**](https://phillipi.github.io/pix2pix/) <br> | CVPR | 2017 | [Code](https://github.com/phillipi/pix2pix)  |  |

## Awesome Data
|  Title  |   Venue  |   Date   |   Code / Info   |   Demo   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| <br> [**HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation**](https://humanoid-bench.github.io/) <br> | arXiv | 2024-06 | [Code](https://github.com/carlosferrazza/humanoid-bench)  |  |
| <br> [**ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions**](https://jlogkim.github.io/parahome/) <br> | arVix  | 2025 | [Code](https://github.com/snuvclab/ParaHome)  |  |
| <br> [**Object motion guided human motion synthesis**](https://lijiaman.github.io/projects/omomo/) <br> | SIGGRAPH Asia | 2023 | [Code](https://github.com/lijiaman/omomo_release)  |  |
| <br> [**3D-FUTURE-ToolBox**](https://arxiv.org/pdf/1906.05797) <br> |  | 2020 | [Code](https://github.com/3D-FRONT-FUTURE/3D-FUTURE-ToolBox)  |  |
| <br> [**Replica Dataset**](https://arxiv.org/pdf/1906.05797) <br> | arVix | 2019 | [Code](https://github.com/facebookresearch/Replica-Dataset)  |  |
| <br> [**RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning**](https://roboverseorg.github.io/) <br> | RSS | 2025 | [Code](https://github.com/RoboVerseOrg/RoboVerse)  |  |
| <br> [**SMPLOlympics: Sports Environments for Physically Simulated Humanoids**](https://smplolympics.github.io/SMPLOlympics-Site/) <br> | arXiv | 2024 | [Code](https://github.com/SMPLOlympics/SMPLOlympics)  |  |
| <br> [**Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware**](https://real2render2real.com/) <br> | arXiv | 2025 | [Code](https://github.com/uynitsuj/real2render2real)  |  |
| <br> [**HUMOTO: A 4D Dataset of Mocap Human Object Interactions**](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://jiaxin-lu.github.io/humoto/&ved=2ahUKEwjG87nnwveNAxX-sFYBHbNdNkMQFnoECAoQAQ&usg=AOvVaw1UkZyiL_RJGQ8sOjn2xxvC) <br> | arXiv | 2025 | |  |

---
# Awesome Reference Source

## Awesome Books
| Name | Link | 
|:-----|:-----:|
|<br> **Introduction to Humanoid Robotics**<br>  | [Link](https://link.springer.com/book/10.1007/978-3-642-54536-8) |

## Awesome Source
| Name | Link | 
|:-----|:-----:|
|<be> [**Spinning Up in Deep RL (OpenAI)**](https://spinningup.openai.com/en/latest/) <br> | [Github](https://github.com/openai/spinningup/tree/master) |
|<be> [**mink**](https://github.com/kevinzakka/mink/) <br> | [Github](https://github.com/kevinzakka/mink/) |
| <br> [**Demonstrating Berkeley Humanoid Lite:An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot**](https://lite.berkeley-humanoid.org/) <br> | [Github](https://github.com/HybridRobotics/Berkeley-Humanoid-Lite) |
